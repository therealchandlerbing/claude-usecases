# Quality Scoring Rubric

Detailed criteria for assessing persona quality and pathways to improvement.

---

## Overview

The 1-5 scoring rubric measures evidence strength, specificity, and actionability. Honest scoring enables targeted improvement and sets appropriate expectations for persona use.

---

## Score Definitions

### Score 5: Exceptional Quality

**Evidence Requirements:**
- 10+ validated interviews per persona
- Direct quotes with source attribution
- Multiple data source triangulation

**Content Standards:**
- Each persona clearly distinct with non-overlapping needs
- Current solutions specifically named with detailed gap analysis
- Pain points quantified (time, cost, frequency)
- All framework sections richly populated
- Behavioral observations specific and dated

**Output Badge:** "Validated - Exceptional Quality"

**Typical Use Cases:**
- High-stakes Vianeo submissions
- Major investment decisions
- Product launch go/no-go
- Strategic partnership commitments

---

### Score 4: Strong Quality

**Evidence Requirements:**
- 5-10 interviews per persona
- Good behavioral detail with validation
- Most claims have interview support

**Content Standards:**
- Personas distinct with minor overlap acceptable
- Current solutions identified with basic gap analysis
- Most pain points specific and measurable
- Framework sections well-populated
- Clear behavioral patterns documented

**Output Badge:** "Validated - Strong Quality"

**Typical Use Cases:**
- Competitive Vianeo submissions
- Product prioritization decisions
- Partnership development
- Internal strategy alignment

---

### Score 3: Adequate Quality

**Evidence Requirements:**
- 3-5 interviews per persona
- Basic behavioral information
- Some claims validated, others inferred

**Content Standards:**
- Personas somewhat distinct
- Current solutions mentioned but not deeply analyzed
- Pain points identified but not all specific
- Framework sections populated at basic level
- Mix of validated and inferred content

**Output Badge:** "Validated - Adequate Quality"

**Typical Use Cases:**
- Early-stage validation
- Internal planning and alignment
- Research direction guidance
- Hypothesis testing preparation

---

### Score 2: Weak Quality

**Evidence Requirements:**
- 1-2 interviews or secondary research only
- Limited behavioral detail
- Most claims inferred from indirect sources

**Content Standards:**
- Significant overlap between personas
- Current solutions vaguely mentioned
- Pain points generic or assumed
- Framework sections incomplete
- Heavy reliance on inference

**Output Badge:** "Inferred - Needs Validation"

**Typical Use Cases:**
- Research planning guidance
- Interview design
- Hypothesis generation
- Early exploration

---

### Score 1: Hypothesis Only

**Evidence Requirements:**
- No interview validation
- Based on assumptions and secondary sources
- No direct stakeholder feedback

**Content Standards:**
- Generic or hypothetical personas
- No clear distinction between segments
- No current solution analysis
- Assumed needs without evidence
- Framework largely incomplete

**Output Badge:** "Not Validated - Hypothesis Only"

**Typical Use Cases:**
- Research planning start
- Initial hypothesis framing
- Interview guide development
- NOT for decisions or submissions

---

## Assessment Criteria Matrix

| Criteria | Score 5 | Score 4 | Score 3 | Score 2 | Score 1 |
|----------|---------|---------|---------|---------|---------|
| **Interviews** | 10+ per persona | 5-10 per persona | 3-5 per persona | 1-2 per persona | 0 |
| **Quotes** | Direct, attributed | Good availability | Some available | Limited | None |
| **Distinctiveness** | Clearly unique | Minor overlap | Somewhat distinct | Significant overlap | Not distinct |
| **Pain Specificity** | Quantified | Mostly specific | Some specific | Generic | Assumed |
| **Current Solutions** | Named + gaps | Identified + basic gaps | Mentioned | Vague | Missing |
| **Behaviors** | Dated, specific | Observable | Basic | Limited | None |

---

## Improvement Pathways

### Score 1 → Score 2
**Primary need:** Get any direct stakeholder feedback

**Actions:**
1. Conduct 1-2 interviews per persona segment
2. Validate or invalidate key assumptions
3. Document at least one current solution
4. Capture specific pain point examples

**Effort:** 4-8 hours of interviews

---

### Score 2 → Score 3
**Primary need:** Increase evidence base and specificity

**Actions:**
1. Conduct 2-3 additional interviews per persona
2. Extract direct quotes for key claims
3. Document specific current solutions
4. Improve pain point specificity
5. Reduce persona overlap through refinement

**Effort:** 6-12 hours of interviews

---

### Score 3 → Score 4
**Primary need:** Deepen evidence and distinctiveness

**Actions:**
1. Conduct 2-5 additional interviews per persona
2. Quantify key pain points (time, cost, frequency)
3. Analyze current solution gaps specifically
4. Ensure personas are clearly distinct
5. Triangulate with behavioral data

**Effort:** 8-16 hours of interviews + analysis

---

### Score 4 → Score 5
**Primary need:** Reach exceptional evidence depth

**Actions:**
1. Conduct 2-5 additional interviews per persona
2. Fully quantify all pain points
3. Deep-dive current solution gap analysis
4. Multiple source triangulation for all claims
5. Date-stamp behavioral observations
6. Ensure zero overlap between personas

**Effort:** 10-20 hours of interviews + analysis

---

## Common Quality Issues and Fixes

### Issue: Overlapping Personas
**Symptoms:** Two personas have similar needs, pains, and behaviors
**Fix:** Consolidate into single persona or identify truly differentiating factors

### Issue: Generic Pain Points
**Symptoms:** "Frustrated" without specific triggers, "Time-consuming" without quantities
**Fix:** Ask follow-up questions: "How often?" "How long?" "What's the cost?"

### Issue: Solution-First Hopes
**Symptoms:** Expectations describe your product, not their outcomes
**Fix:** Reframe as outcomes: "What would success look like regardless of solution?"

### Issue: Missing Layer 4
**Symptoms:** No current solution analysis
**Fix:** Ask explicitly: "What do you use now?" "Why doesn't that work?"

### Issue: Assumed Motivations
**Symptoms:** Life/Motivations without interview evidence
**Fix:** Ask about goals, career drivers, what success means to them

### Issue: Vague Others Say
**Symptoms:** Generic perceptions without actual sources
**Fix:** Ask: "What feedback have you received?" "What does your team say?"

---

## Validation Evidence Types

### Strong Evidence (Supports Score 4-5)
- Direct interview quotes
- Survey responses with open text
- Observed behaviors during sessions
- Documented decisions and outcomes
- Multiple source confirmation

### Moderate Evidence (Supports Score 3-4)
- Summarized interview notes
- Behavioral patterns from data
- Stakeholder feedback reports
- Project documentation

### Weak Evidence (Supports Score 2-3)
- Secondary research
- Analogous user studies
- Expert assumptions
- Industry benchmarks

### No Evidence (Score 1)
- Assumptions without validation
- Hypotheses without testing
- Generic personas from templates

---

## Scoring FAQs

### Q: Should I round up or down?
**A:** Round to the score that best reflects evidence strength. When in doubt, score conservatively—it's better to under-promise and over-deliver.

### Q: What if personas are different scores?
**A:** Score each persona individually. Report the range and overall set quality.

### Q: Can I submit Score 3 to Vianeo?
**A:** Score 3 is adequate but not competitive. For important submissions, target Score 4+.

### Q: How do I count interviews if one person is in multiple personas?
**A:** Count the interview once per persona it primarily informs. Cross-references are secondary evidence.

### Q: What about behavioral data without interviews?
**A:** Behavioral data supports inference but doesn't replace validation. Mark as "Inferred from [source]."

---

## Using Scores for Decisions

### Score 5
- Ready for high-stakes decisions
- Suitable for external presentations
- Can drive major investments

### Score 4
- Ready for strategic decisions
- Suitable for competitive submissions
- Can inform product priorities

### Score 3
- Ready for internal planning
- Guides further research
- Use with appropriate caveats

### Score 2
- Guides research design
- Identifies key questions
- NOT ready for decisions

### Score 1
- Starting point only
- Hypothesis to validate
- NOT for any decisions

---

**Version:** 1.0.0 | **Last Updated:** 2025-11-19
