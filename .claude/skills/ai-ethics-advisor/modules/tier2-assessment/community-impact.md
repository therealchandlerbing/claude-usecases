# Community Impact Assessment

Evaluating how AI systems affect communities, power dynamics, accessibility, and collective wellbeing.

## Affected Community Engagement

Meaningful community engagement is not optional for ethical AI—it's essential. Those who bear the consequences of AI systems must have a voice in their design and deployment.

### Have affected communities been consulted?

**Who constitutes "affected communities"?**
- Direct users of the system
- Those subject to AI decisions (even if they don't interact directly)
- Communities disproportionately impacted
- Vulnerable populations at heightened risk
- Workers whose jobs may be affected
- Broader society if systemic implications exist

**Consultation Requirements:**
- **Early:** Before major design decisions are locked in
- **Ongoing:** Not one-time, but throughout development and deployment
- **Genuine:** Real influence, not token consultation
- **Accessible:** Methods appropriate to communities (language, format, location)
- **Compensated:** Recognize and pay for expertise and time

**Methods:**
- Community advisory boards
- Focus groups and interviews
- Surveys and public comment periods
- Participatory design workshops
- Deliberative forums
- Community-based participatory research

### Are community perspectives reflected in design?

**Evidence of Integration:**
- Design changes made based on community feedback
- Community priorities reflected in system goals
- Features requested by community included
- Concerns raised by community addressed
- Transparent documentation of how feedback shaped decisions

**Red Flags:**
- Community consultation happened but no changes resulted
- Feedback collected but not acted upon
- Design team can't articulate how community input influenced decisions
- Community members don't recognize their input in final system

**Accountability:**
- Report back to communities on how input was used
- Explain why suggestions were or weren't incorporated
- Ongoing relationship, not extractive consultation
- Community members can verify their influence

### Do communities understand how the system affects them?

**Transparency Requirements:**
- Clear communication about AI role in decisions
- Plain-language explanation of how system works
- Honest about what data is used and how
- Transparent about limitations and potential harms
- Multiple formats and languages as needed

**Understanding Assessment:**
- Can community members explain what the system does?
- Do they know when and how it affects them?
- Do they understand their rights and options?
- Can they identify potential harms?

**Information Provision:**
- Public documentation accessible to laypersons
- Community meetings and presentations
- Educational materials in relevant languages
- Responsive to questions and concerns
- Regular updates about system changes

### Can communities provide ongoing feedback?

**Feedback Mechanisms:**
- Easy ways to report concerns or issues
- Multiple channels (web, phone, in-person, mail)
- Accessible to people with disabilities
- Available in relevant languages
- Anonymous options if needed
- Clear process and timeline for response

**Responsiveness:**
- Feedback actually reviewed and considered
- Patterns in feedback trigger investigations
- Changes made in response to feedback
- Transparent reporting on feedback and responses
- Community can see impact of their input

**Feedback Loop Closure:**
- Community told what happened with their feedback
- Explanations when feedback can't be acted upon
- Gratitude and recognition for input
- Ongoing relationship building

## Power Dynamics Analysis

AI systems don't just process data—they redistribute power. Critical to assess who gains and who loses power.

### Does the system shift power relationships?

**Power Shifts to Assess:**

**Decision-Making Power:**
- Who made decisions before AI?
- Who makes them now?
- Whose discretion is reduced?
- Whose authority is enhanced?

**Example:**
- Before: Teachers assign grades based on holistic assessment
- After: Algorithm determines grades, teacher input reduced
- Power shift: From teachers to system designers and administrators

**Information Power:**
- Who has access to information and insights?
- Whose knowledge is valued or devalued?
- Who controls data and algorithms?
- Whose expertise is amplified or diminished?

**Economic Power:**
- Who benefits financially?
- Whose economic opportunities are affected?
- How are resources and opportunities redistributed?
- Who bears economic risks?

### Who gains decision-making authority?

**Authority Gainers:**
- System designers and developers
- Data providers and collectors
- Algorithm owners and operators
- Centralized decision-makers
- Technical experts

**Form of Authority:**
- Direct control over algorithm
- Interpretation of outputs
- Setting thresholds and parameters
- Access to data and insights
- Power to override or modify

**Concentration Risk:**
- Is decision-making more centralized?
- Are fewer people making more consequential decisions?
- Is authority less visible or accountable?
- Can authority be contested or checked?

### Who loses agency or control?

**Agency Losers:**
- Those subject to AI decisions (but who don't control the system)
- Front-line workers whose discretion is reduced
- Communities affected but not consulted
- Individuals with less data literacy
- Those without access to technical expertise to contest

**Forms of Lost Agency:**
- Reduced ability to influence outcomes
- Less discretion in professional judgment
- Fewer options and choices
- Difficulty understanding or contesting decisions
- Powerlessness in face of automated systems

**Particularly Vulnerable:**
- Low-income individuals (less ability to opt out or contest)
- Marginalized communities (less power to resist)
- Non-technical users (can't understand or challenge)
- Those with less education (information asymmetry)
- People in rural areas (fewer alternatives)

### Are existing inequities amplified or reduced?

**Amplification of Inequity:**

Systems often amplify existing inequities when:
- Historical bias is encoded in training data
- Feedback loops reinforce disadvantage
- Those already marginalized face additional barriers
- Power concentrates further
- Access gaps widen

**Examples:**
- Predictive policing concentrating in already over-policed areas
- Hiring algorithms trained on historically biased decisions
- Credit scoring that disadvantages those with limited credit history
- Educational algorithms tracking based on past inequities

**Reduction of Inequity:**

Systems can reduce inequity when:
- They remove human bias in decisions
- They expand access to opportunities
- They level information asymmetries
- They reduce barriers for marginalized groups
- Designed explicitly for equity

**Examples:**
- Blind auditions/reviews removing identity-based bias
- Assistive technology expanding access
- Automated translation breaking language barriers
- Systems designed to detect and correct human bias

**Critical Assessment:**
Most systems claim to reduce inequity but actually amplify it. Requires rigorous testing and evidence, not assumptions.

## Accessibility & Inclusion

AI systems must work for everyone, not just majority or privileged users.

### Is the system accessible to people with disabilities?

**Accessibility Dimensions:**

**Visual Accessibility:**
- Screen reader compatibility
- Alternative text for images
- Sufficient color contrast
- Adjustable font sizes
- Audio alternatives for visual information

**Auditory Accessibility:**
- Captions and transcripts for audio
- Visual alternatives for audio cues
- Sign language interpretation
- Written versions of voice interactions

**Motor Accessibility:**
- Keyboard navigation (no mouse required)
- Voice control options
- Adjustable timing (no automatic timeouts)
- Simple interactions, no complex gestures
- Switch access and other assistive tech support

**Cognitive Accessibility:**
- Clear, simple language
- Consistent interface and interactions
- Ample time to complete tasks
- Error prevention and clear error messages
- Help and guidance readily available

**Testing:**
- Automated accessibility testing (WCAG compliance)
- Testing with assistive technologies
- User testing with people with disabilities
- Compliance with Section 508, ADA, accessibility standards

### Does it work across languages and literacy levels?

**Language Accessibility:**
- Available in languages spoken by affected communities
- Professional translation, not just machine translation
- Cultural adaptation, not just literal translation
- Right-to-left languages supported if relevant
- Multilingual support, not English-only

**Literacy Considerations:**
- Plain language (6th-8th grade reading level ideal)
- Visual aids and icons to supplement text
- Audio options for low-literacy users
- Simple, clear interfaces
- Help and explanations readily available
- No jargon or technical terms without explanation

**Numeracy:**
- Clear presentation of numbers and statistics
- Visual representations (charts, graphs)
- Context and interpretation, not just raw numbers
- Avoid complex calculations required from users

### Are interface and interaction patterns culturally appropriate?

**Cultural Appropriateness:**

**Visual Design:**
- Colors have different cultural meanings
- Images and icons may be culture-specific
- Layout preferences vary (not all cultures read left-to-right)
- Imagery should represent diverse communities

**Interaction Patterns:**
- Some cultures prefer direct communication, others indirect
- Formality levels vary
- Personal information requests may be sensitive
- Collectivist vs. individualist framing

**Content:**
- Examples relevant across cultures
- Avoid culture-specific references without explanation
- Names accommodate diverse formats
- Dates, times, addresses support international formats

**Values and Assumptions:**
- Don't assume Western, educated, industrialized, rich, democratic (WEIRD) norms
- Consider different family structures
- Respect varying concepts of privacy
- Acknowledge diverse economic situations

### Can marginalized communities meaningfully engage with the system?

**Engagement Barriers to Address:**

**Technology Access:**
- Do affected communities have necessary devices?
- Is internet access reliable and affordable?
- Are data costs prohibitive?
- Are public access points available?

**Digital Literacy:**
- What is the technical skill level required?
- Is training and support provided?
- Are there simpler alternatives for less tech-savvy users?
- Is human assistance available?

**Trust:**
- Do marginalized communities trust the system and organization?
- Is there history of surveillance or discrimination?
- Are privacy protections strong enough?
- Is there community representation in governance?

**Power and Voice:**
- Can communities influence the system?
- Are feedback mechanisms genuinely responsive?
- Is there accountability to communities?
- Can communities organize and advocate?

## Collective Impact

Beyond individual effects, assess impacts on communities as a whole.

### How does this affect community cohesion?

**Potential Negative Impacts:**
- Creating divisions or categories that didn't exist
- Differential treatment breeding resentment
- Surveillance undermining trust
- Reducing human interaction and connection
- Stigmatizing certain groups

**Potential Positive Impacts:**
- Connecting people with resources and services
- Enabling communication and coordination
- Reducing discrimination through standardization
- Creating shared understanding or awareness

**Assessment Questions:**
- Does this bring community together or drive wedges?
- Does it create new categories or hierarchies?
- How does it affect trust within community?
- Does it support or undermine social bonds?

### Does it enable or hinder collective action?

**Collective Action Considerations:**

**Enablement:**
- Helps people organize and coordinate?
- Provides information for advocacy?
- Reduces barriers to participation?
- Amplifies marginalized voices?

**Hindrance:**
- Surveils or tracks organizing?
- Punishes collective action?
- Divides communities?
- Concentrates power away from grassroots?

**Particular Concern:**
Systems that could be used to identify, track, or suppress:
- Union organizing
- Political organizing
- Protests and demonstrations
- Community advocacy
- Whistleblowing

### What are spillover effects on non-users?

**Non-User Impacts:**

Even people who don't use the system may be affected:
- Decisions about them made using AI
- Community norms and expectations shift
- Services become AI-dependent
- Opting out becomes difficult or impossible
- Data about them collected indirectly

**Network Effects:**
- Your data influences decisions about others
- Others' data influences decisions about you
- Community-level patterns affect individuals
- Collective privacy violations

**Coercion to Adopt:**
- Is use effectively mandatory?
- Can people opt out without significant cost?
- Are non-users disadvantaged?
- Is there meaningful choice?

### How does it shape social norms and expectations?

**Normalization:**
AI systems shape what's considered normal, acceptable, or expected:
- Surveillance becomes normalized
- Certain metrics become valued over others
- Standardization crowds out diversity
- Efficiency prioritized over human connection
- Quantification of aspects of life previously qualitative

**Long-Term Cultural Shifts:**
- What behaviors does this incentivize or discourage?
- How does it shape identity and self-perception?
- What values does it implicitly promote?
- How might it change community culture over time?

**Positive Normalization:**
- Accessibility becomes standard
- Bias reduction becomes expected
- Privacy protection normalized
- Equity considerations standard practice

**Negative Normalization:**
- Constant surveillance accepted
- Algorithmic decision-making unquestioned
- Privacy erosion normalized
- Discrimination by algorithm more acceptable than by humans

## Community Impact Assessment Output

```
COMMUNITY IMPACT ASSESSMENT SUMMARY

COMMUNITY ENGAGEMENT
□ Affected communities consulted: [Yes/No, details]
□ Community perspectives reflected in design: [Yes/No, how]
□ Communities understand system impact: [Assessment]
□ Ongoing feedback mechanisms: [Yes/No, description]

POWER DYNAMICS
□ Power shifts: [Description of who gains/loses authority]
□ Decision-making centralization: [Assessment]
□ Agency impacts: [Who loses control, how significant]
□ Equity impact: [Amplifies or reduces existing inequities]

ACCESSIBILITY & INCLUSION
□ Disability accessibility: [Level of compliance]
□ Language accessibility: [Languages supported, literacy level]
□ Cultural appropriateness: [Assessment]
□ Marginalized community engagement: [Meaningful/Tokenistic/Absent]

COLLECTIVE IMPACT
□ Community cohesion: [Strengthened/Neutral/Weakened]
□ Collective action: [Enabled/Neutral/Hindered]
□ Non-user impacts: [Description]
□ Social norm shifts: [Positive/Negative/Mixed]

OVERALL COMMUNITY IMPACT: [Highly Positive / Positive / Mixed / Negative / Highly Negative]

CRITICAL COMMUNITY CONCERNS:
1. [Most significant community impact issue]
2. [Second priority]
3. [Third priority]

COMMUNITY RECOMMENDATIONS:
[What affected communities have said they want/need]
```

## Principles for Community-Centered AI

**1. Nothing About Us Without Us**
Communities affected by AI must be meaningfully involved in its design, deployment, and governance.

**2. Community Benefit**
AI should serve community needs and priorities, not just organizational efficiency.

**3. Power Redistribution**
AI should empower marginalized communities, not concentrate power further.

**4. Universal Design**
Design for the most marginalized first; it will work better for everyone.

**5. Collective Rights**
Recognize community-level impacts and collective rights, not just individual impacts.

**6. Ongoing Relationship**
Community engagement is ongoing relationship, not one-time consultation.

**7. Accountability to Communities**
Organizations deploying AI should be accountable to affected communities, not just to shareholders or regulators.
